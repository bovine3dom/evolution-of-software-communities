{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"init.jl\")\n",
    "using TensorOperations\n",
    "using TensorDecompositions\n",
    "using Statistics\n",
    "using LinearAlgebra\n",
    "import Plots\n",
    "import StatsPlots\n",
    "import LightGraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor decompositions sort of explained for Colin\n",
    "\n",
    "This little notebook is for me to play with the tensor packages in Julia; to try to remember some stuff from my degree; and to help Colin understand what the point of it all is.\n",
    "\n",
    "First things first - there is a bug in Julia where `zeroes` is misspelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroes = zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating our tensor\n",
    "\n",
    "The idea behind a tensor decomposition or factorisation is that there's a lot of degeneracy around in the world these days.\n",
    "\n",
    "Say, for example, you generate a tensor from three vectors. Then, by definition, that tensor can be described by three vectors rather than having to describe the whole tensor.\n",
    "\n",
    "The point of this is that the three vectors are more interesting than the tensor - they will describe the patterns that have led to that tensor being created.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$ a_{ijk} = \\Sigma_{r=1}^R u_{ir} v_{jr} w_{kr} $$\n",
    "\n",
    "where $R$ is the number of factors asked for in the algorithm.\n",
    "\n",
    "In this notebook, we'll generate a tensor $a$ from $u,v,w$, and use a tensor decomposition to attempt to retrieve those vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = [1000,200,30,3]\n",
    "u,v,w,x = [rand(dim) for dim in DIMS]\n",
    "z = rand(DIMS...)\n",
    "a = zeroes(DIMS...)\n",
    "@show a |> size\n",
    "\n",
    "# This is using Einstein's summation convention - the whole tensor a is generated by iterating through all possible pairs \n",
    "@tensor a[i,j,k,l] = u[i] * v[j] * w[k] * x[l]# + 0.001*z[i,j,k]\n",
    "@show mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F = candecomp(a, 1, (randn(10,1),randn(20,1),randn(30,1));\n",
    "#    # NB: need to have 2D arrays for the guess; 1D isn't good enough\n",
    "#    compute_error=true,\n",
    "#    method=:ALS,\n",
    "#    maxiter=100_000,\n",
    "#);\n",
    "F = nncp(a, 1;\n",
    "    compute_error=true,\n",
    "    maxiter=10,\n",
    ");\n",
    "F.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversely, tensor operations wants 1D arrays\n",
    "U,V,W,X = [f |> Iterators.flatten |> collect for f in F.factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tensor A[i,j,k,l] := U[i] * V[j] * W[k] * X[l];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show mean(abs.(A-a))\n",
    "@show mean(a)\n",
    "@show mean(A)\n",
    "inds = rand(1:minimum(DIMS))\n",
    "@show A[inds...]\n",
    "@show a[inds...]\n",
    "@show maximum(abs.(A-a))\n",
    "\n",
    "# Nice. Easy-mode.\n",
    "# NB: u,v,w aren't the same as U,V,W; but a and A are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsPlots.plot([StatsPlots.boxplot(vec,legend=:none) for vec in (u,U,v,V,w,W)]..., layout=(1,6))\n",
    "\n",
    "# This is quite interesting: it gets the shapes of the distributions right, but the normalisation wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what does this mean for graphs?\n",
    "\n",
    "First, let's make an adjacency matrix that varies with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make an undirected graph with no self-loops as our base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "TIMESTEPS = 10\n",
    "DENSITY = 0.05 # unused atm\n",
    "LINK_HIATUS_RATE = 0.1\n",
    "#base_adj = Int.(rand(N,N) .< DENSITY) |> Symmetric\n",
    "#base_adj = base_adj .& mod.(one(base_adj).+1,2) # get rid of self-loops;\n",
    "\n",
    "# Let's make an actually interesting graph\n",
    "#g = LightGraphs.barabasi_albert(N,1)\n",
    "#g = LightGraphs.erdos_renyi(N,DENSITY)\n",
    "g = LightGraphs.watts_strogatz(N,4,0.01)\n",
    "base_adj = g |> LightGraphs.adjacency_matrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timestep, we temporarily turn off LINK_HIATUS_RATE links, and make a nice tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ProgressMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perms = Int.(rand(N,N,TIMESTEPS) .> LINK_HIATUS_RATE)\n",
    "for t in 1:TIMESTEPS\n",
    "    perms[:,:,t] = perms[:,:,t]  |> Symmetric\n",
    "end\n",
    "\n",
    "# Want to make a more recognisable temporal pattern\n",
    "# Mask a block off and turn it on and off at various times\n",
    " perms = zeroes(Int,N,N,TIMESTEPS)\n",
    "SPECIAL_NODES = N ÷ 10\n",
    "SPECIAL_TIME = 3\n",
    "for i in 1:N, j in 1:N, t in 1:TIMESTEPS\n",
    "    perms[i,j,t] = i <= SPECIAL_NODES && j <= SPECIAL_NODES && t <= SPECIAL_TIME ? 0 : 1\n",
    "end\n",
    "# should probably find a nicer way of doing this...\n",
    "\n",
    "ADJ = zeroes(N,N,TIMESTEPS) # making this takes _AGES_\n",
    "for i in 1:N, j in 1:N, t in 1:TIMESTEPS\n",
    "    ADJ[i,j,t] = base_adj[i,j] & perms[i,j,t]\n",
    "end\n",
    "\n",
    "#ProgressMeter.@showprogress for t in 1:TIMESTEPS\n",
    "#    ADJ[:,:,t] = base_adj\n",
    "#end\n",
    "\n",
    "# I expected\n",
    "# @tensor ADJ[i,j,t] = base_adj[i,j] & perms[i,j,t]\n",
    "# to work, but it doesn't.\n",
    "\n",
    "ADJ |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods(nncp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 10\n",
    "@time F = nncp(ADJ, R; # R is the number of components ≈ number of communities\n",
    "    # I think Ciro spoke about optimising this number somewhere\n",
    "    # papers refer to it - core consistency\n",
    "    compute_error=true,\n",
    "    maxiter=1000,\n",
    ");\n",
    "# (each u*v*w[:,r] is a component; each of u,v,w is a factor)\n",
    "# n = 1000 -> 2 seconds\n",
    "# n = 10,000 -> bloody ages. severely limited by i/o - using 25% CPU only. has been like, 5 minutes. Am bored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.factors[3] |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.factors[1] # The largest numbers in each nodes correspond to the nodes most active in that community\n",
    "(u,v,w) = [f for f in F.factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plot(F.factors[3]) # You can see that some communities are inactive for the first few timesteps, as we'd expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 bestest nodes?\n",
    "nbest = (n,v) ->  [findfirst(x->x==f,v) for f\n",
    "        in sort(v,rev=true)[1:n]\n",
    "]\n",
    "@show nbest(5,u[:,1])\n",
    "@show nbest(5,v[:,1])\n",
    "\n",
    "# Nodes that have high scores in the first vector of the first factor\n",
    "# Tend to link to nodes with high scores in the second vector of the\n",
    "# first factor\n",
    "# and tend to link to things at high-scoring times in the third vector\n",
    "\n",
    "# Which sounds weirdly bi-partite?\n",
    "\n",
    "# So, a \"community\" is this:\n",
    "# Nodes which link to similar nodes\n",
    "# Nodes which have most of their links turned on at high-scoring times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.heatmap(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GraphPlot\n",
    "import Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_size = N ÷ R\n",
    "colours = Colors.distinguishable_colors(R,Colors.colorant\"blue\")\n",
    "nodefillarr = []\n",
    "for n in 1:N\n",
    "    ind = findmax(u[n,:])[2]\n",
    "    push!(nodefillarr,colours[ind])\n",
    "end\n",
    "GraphPlot.gplot(g;nodefillc=nodefillarr) # Sure, these kind of look like communities.\n",
    "# Eugh, if you put too many nodes in, GraphPlot gives up trying to colour them\n",
    "# Would be nice to colour nodes by strength of their association too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours # Key - 1:R+1 from left to right (last is uncategorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodefillarr |> unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network interpretation\n",
    "\n",
    "Say you have a temporal adjacency matrix which you wish to decompose into $R$ communities:\n",
    "\n",
    "$$ a_{ijt} = \\Sigma_{r=1}^R u_{ir} v_{jr} w_{tr} $$\n",
    "\n",
    "NB: Each of $u,v,w$ is roughly (self) column-wise orthogonal - i.e, if an entry is large in one column of, e.g, u, then it will be smaller in another column of u.\n",
    "\n",
    "Therefore, for any edge $a_{ijt}$ to appear from node $j$ to $i$ at time $t$, it must be true that, for some $r$, $u_{ir} v_{jr} w_{tr} \\approx 1$.\n",
    "\n",
    "Ignoring the temporal factor for a moment, we can say that high-scoring nodes in $\\boldsymbol{u}_{r}$ are probably linked to by nodes in $\\boldsymbol{v}_{r}$ - all at once, i.e, each $r$ fuzzily denotes a community of nodes who link to each other: so, $u_{ir}$ denotes the strength\n",
    "\n",
    "$w_{tr}$ has a similar meaning: the highest scoring times for each $r$ means that the high-scoring nodes in $u$ and $v$ are \"turned on\" and have more outgoing or incoming links at those times.\n",
    "\n",
    "In an undirected graph, we expect $u$ and $v$ to look similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fiddling\n",
    "\n",
    "You don't need to pay attention to stuff below this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = u * v'\n",
    "@tensor a[i,j,k] = b[i,j] * w[k] + 0.001*z[i,j,k]\n",
    "\n",
    "fixme = ones(1:TIMESTEPS)\n",
    "@tensor testADJ[i,j,t] := base_adj[i,j] * fixme[t] + perms[i,j,t]\n",
    "\n",
    "# I really don't understand the difference between these expressions\n",
    "# Or why fixme is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show findfirst(x->false,[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays\n",
    "\n",
    "DIMS = [10,20,30]\n",
    "u,v,w = [round.(rand(dim)) for dim in DIMS] .|> hcat .|> SparseMatrixCSC # rounding should actually be done later\n",
    "z = rand(DIMS...)\n",
    "a = zeroes(DIMS...) # |> sparse # Error - sparse tensors aren't in Julia. Consider adopting https://github.com/JuliaTensors/SparseTensors.jl ?\n",
    "# Or we could write a wrapper for taco - https://github.com/tensor-compiler/taco\n",
    "@show a |> size\n",
    "\n",
    "# This is using Einstein's summation convention - the whole tensor a is generated by iterating through all possible pairs \n",
    "#@tensor a[i,j,k] = u[i] * v[j] * w[k]# + 0.001*z[i,j,k]\n",
    "for i in 1:DIMS[1], j in 1:DIMS[2], k in 1:DIMS[3]\n",
    "    a[i,j,k] = u[i] * v[j] * w[k]\n",
    "end\n",
    "@show mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_adj |> Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "Ns = 10 .^(1:0.1:4) .|> round .|> Int\n",
    "R = 10\n",
    "for n in Ns\n",
    "    ADJ = zeroes(n,n,1) # making this takes _AGES_\n",
    "    g = LightGraphs.watts_strogatz(n,4,0.01)\n",
    "    base_adj = g |> LightGraphs.adjacency_matrix;\n",
    "    ADJ[:,:,1] = base_adj\n",
    "\n",
    "    t = @elapsed F = nncp(ADJ, 1; # R is the number of components ≈ number of communities\n",
    "        # I think Ciro spoke about optimising this number somewhere\n",
    "        # papers refer to it - core consistency\n",
    "        compute_error=true,\n",
    "        maxiter=100,\n",
    "    );\n",
    "    push!(times,t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.scatter(Ns,times;\n",
    "    xlabel = \"N\",\n",
    "    ylabel = \"Time taken / seconds\",\n",
    "    legend = :none,\n",
    "    yscale = :log10,\n",
    "    xscale = :log10\n",
    ")\n",
    "\n",
    "# tl;dr: power-law, N^k, where k is quite big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing R\n",
    "\n",
    "https://github.com/alessandrobessi/corcondia/blob/master/coreconsistency.py <- CONCORDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BenchmarkTools\n",
    "delta = δ(args...) = reduce(==,args) |> Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchmarkTools.@benchmark delta(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCORDIA\n",
    "c = 0\n",
    "for i in 1:N, j in 1:N, t in 1:TIMESTEPS\n",
    "    temp = sum([u[i,r] + v[j,r] + w[t,r] for r in 1:R])\n",
    "    c += (temp - δ(i,j,t))^2\n",
    "end\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = []\n",
    "V = []\n",
    "Σ = []\n",
    "for m in [u,v,w]\n",
    "    svds = svd(m)\n",
    "    push!(U,svds.U)\n",
    "    push!(V,svds.V)\n",
    "    push!(Σ,svds.S |> Diagonal)\n",
    "end\n",
    "rADJ = zeroes(N,N,TIMESTEPS)\n",
    "for i in 1:N, j in 1:N, t in 1:TIMESTEPS\n",
    "    rADJ[i,j,t] = u[i] * v[j] * w[t]\n",
    "end\n",
    "rADJ |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kron(transpose.(U)..., rADJ[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "julia"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.x",
   "language": "julia",
   "name": "julia-1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
