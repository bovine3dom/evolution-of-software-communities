{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"init.jl\")\n",
    "using TensorOperations\n",
    "using TensorDecompositions\n",
    "using Statistics\n",
    "using LinearAlgebra\n",
    "import Plots\n",
    "import StatsPlots\n",
    "import LightGraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor decompositions sort of explained for Colin\n",
    "\n",
    "This little notebook is for me to play with the tensor packages in Julia; to try to remember some stuff from my degree; and to help Colin understand what the point of it all is.\n",
    "\n",
    "First things first - there is a bug in Julia where `zeroes` is misspelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroes = zeros\n",
    "\n",
    "# to-do:\n",
    "# fork nncp to give it a quieter option (disabling progress bar); verbose=false does some\n",
    "# also make nncp quick\n",
    "# look into corcondia scaling with R\n",
    "# check corcondia code is correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating our tensor\n",
    "\n",
    "The idea behind a tensor decomposition or factorisation is that there's a lot of degeneracy around in the world these days.\n",
    "\n",
    "Say, for example, you generate a tensor from three vectors. Then, by definition, that tensor can be described by three vectors rather than having to describe the whole tensor.\n",
    "\n",
    "The point of this is that the three vectors are more interesting than the tensor - they will describe the patterns that have led to that tensor being created.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$ a_{ijk} = \\Sigma_{r=1}^R u_{ir} v_{jr} w_{kr} $$\n",
    "\n",
    "where $R$ is the number of factors asked for in the algorithm.\n",
    "\n",
    "In this notebook, we'll generate a tensor $a$ from $u,v,w$, and use a tensor decomposition to attempt to retrieve those vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = [100,20,10,3]\n",
    "u,v,w,x = [rand(dim) for dim in DIMS]\n",
    "z = rand(DIMS...)\n",
    "a = zeroes(DIMS...)\n",
    "@show a |> size\n",
    "\n",
    "# This is using Einstein's summation convention - the whole tensor a is generated by iterating through all possible pairs \n",
    "@tensor a[i,j,k,l] = u[i] * v[j] * w[k] * x[l]# + 0.001*z[i,j,k]\n",
    "@show mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F = candecomp(a, 1, (randn(10,1),randn(20,1),randn(30,1));\n",
    "#     # NB: need to have 2D arrays for the guess; 1D isn't good enough\n",
    "#     compute_error=true,\n",
    "#     method=:ALS,\n",
    "#     maxiter=100_000,\n",
    "# );\n",
    "F = nncp(a, 1;\n",
    "    compute_error=true,\n",
    "    maxiter=10,\n",
    ");\n",
    "F.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[all(f .> 0) for f in F.factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversely, tensor operations wants 1D arrays\n",
    "U,V,W,X = [f |> Iterators.flatten |> collect for f in F.factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tensor A[i,j,k,l] := U[i] * V[j] * W[k] * X[l];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show mean(abs.(A-a))\n",
    "@show mean(a)\n",
    "@show mean(A)\n",
    "inds = rand(1:minimum(DIMS))\n",
    "@show A[inds...]\n",
    "@show a[inds...]\n",
    "@show maximum(abs.(A-a))\n",
    "\n",
    "# Nice. Easy-mode.\n",
    "# NB: u,v,w aren't the same as U,V,W; but a and A are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsPlots.plot([StatsPlots.boxplot(vec,legend=:none) for vec in (u,U,v,V,w,W)]..., layout=(1,6))\n",
    "\n",
    "# This is quite interesting: it gets the shapes of the distributions right, but the normalisation wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what does this mean for graphs?\n",
    "\n",
    "First, let's make an adjacency matrix that varies with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make an undirected graph with no self-loops as our base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # ignored if using SBM\n",
    "TIMESTEPS = 10\n",
    "DENSITY = 0.05 # unused atm\n",
    "LINK_HIATUS_RATE = 0.1\n",
    "#base_adj = Int.(rand(N,N) .< DENSITY) |> Symmetric\n",
    "#base_adj = base_adj .& mod.(one(base_adj).+1,2) # get rid of self-loops;\n",
    "\n",
    "# Let's make an actually interesting graph\n",
    "# g = LightGraphs.barabasi_albert(N,2)\n",
    "#g = LightGraphs.erdos_renyi(N,DENSITY)\n",
    "g = LightGraphs.watts_strogatz(N,4,0.01)\n",
    "\n",
    "# g = LightGraphs.stochastic_block_model(\n",
    "#     [\n",
    "#         3   2/N   2/N; # c[a,b] = mean number of neighbours between nodes in block a and block b (only pay attention to upper tri)\n",
    "#         0   3   2/N;\n",
    "#         0   0   3;\n",
    "#     ],\n",
    "#     [50,50,50] # number of nodes in each block\n",
    "# )\n",
    "\n",
    "N = LightGraphs.nv(g)\n",
    "base_adj = g |> LightGraphs.adjacency_matrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timestep, we temporarily turn off LINK_HIATUS_RATE links, and make a nice tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ProgressMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perms = Int.(rand(N,N,TIMESTEPS) .> LINK_HIATUS_RATE)\n",
    "for t in 1:TIMESTEPS\n",
    "    perms[:,:,t] = perms[:,:,t]  |> Symmetric\n",
    "end\n",
    "\n",
    "# Want to make a more recognisable temporal pattern\n",
    "# Mask a block off and turn it on and off at various times\n",
    " perms = zeroes(Int,N,N,TIMESTEPS)\n",
    "SPECIAL_NODES = N ÷ 10\n",
    "SPECIAL_TIME = 3\n",
    "#for i in 1:N, j in 1:N, t in 1:TIMESTEPS\n",
    "#    perms[i,j,t] = i <= SPECIAL_NODES && j <= SPECIAL_NODES && t <= SPECIAL_TIME ? 0 : 1\n",
    "#end\n",
    "# should probably find a nicer way of doing this...\n",
    "\n",
    "ADJ = zeroes(N,N,TIMESTEPS) # making this takes _AGES_\n",
    "#for i in 1:N, j in 1:N, t in 1:TIMESTEPS\n",
    "#    ADJ[i,j,t] = base_adj[i,j] & perms[i,j,t]\n",
    "#end\n",
    "\n",
    "for t in 1:TIMESTEPS\n",
    "    ADJ[:,:,t] = base_adj\n",
    "end\n",
    "\n",
    "# I expected\n",
    "# @tensor ADJ[i,j,t] = base_adj[i,j] & perms[i,j,t]\n",
    "# to work, but it doesn't.\n",
    "\n",
    "# should probably try to make more distinct temporal communities that agree with SBM to make this a fairer test\n",
    "\n",
    "ADJ |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TensorToolbox\n",
    "function quick_kron(ts,X)\n",
    "    i_x = 1:(size(X) |> length) |> collect\n",
    "    Y = similar(X)\n",
    "    for (k,t) in reverse(ts |> enumerate |> collect)\n",
    "        Y = ttm(X,t,k)\n",
    "        X = permutedims(Y,reverse(i_x))\n",
    "    end\n",
    "    Y\n",
    "end\n",
    "    \n",
    "function CORCONDIER(X,factors,R)\n",
    "    us = []\n",
    "    ss = []\n",
    "    vs = []\n",
    "#   @show factors\n",
    "#   k = 0.70710678\n",
    "#   factors = [ones(2,2), ones(2,2).*k, ones(2,2).*k]\n",
    "#   @show factors\n",
    "# python gives us different factors\n",
    "    # that explains our discrepancy in corcondia\n",
    "    for f in factors\n",
    "        (u,s,v) = svd(f)\n",
    "        push!(us,u)\n",
    "        push!(ss,s)\n",
    "        push!(vs,v)\n",
    "    end\n",
    "\n",
    "    Y = quick_kron(transpose.(us),X)\n",
    "    # colin claims quick_kron is identical to slow\n",
    "    # provided you reverse the order of the matrices\n",
    "    Z = quick_kron((inv ∘ Diagonal).(ss),Y)\n",
    "    #inds = [R for _ in 1:length(factors)]\n",
    "    G = quick_kron(vs,Z)\n",
    "#   @show get_G(X, factors)\n",
    "    # all this is equivalent to G = reshape(reduce(kron, factors) |> pinv * X[:], inds...)\n",
    "    \n",
    "    # people often do this * 100 \"to make it into percent\"\n",
    "    # but I find it easier to read as a decimal\n",
    "    (1 - mapreduce(k -> (G[k] - δ(Tuple(k)...))^2,+,pairs(IndexCartesian(),G)|>keys)/R)\n",
    "    #[G[i,j,k] - δ(i,j,k) for i in 1:R, j in 1:R, k in 1:R].^2 |> sum\n",
    "    # how the dickens do we reduce this?\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 3\n",
    "F = nncp(ADJ, r; # R is the number of components ≈ number of communities\n",
    "    # I think Ciro spoke about optimising this number somewhere\n",
    "    # papers refer to it - core consistency\n",
    "    compute_error=true,\n",
    "    maxiter=1000,\n",
    "    verbose=false\n",
    ");\n",
    "#methods(nncp)\n",
    "CORCONDIER(ADJ,F.factors,r)\n",
    "\n",
    "# getting NaN on real data, fabulous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Serialization\n",
    "adj_mats = deserialize(\"../data/processed/elm-adj-mats.jls\")\n",
    "X = Array{Float64}(undef, (size(adj_mats[1])..., length(adj_mats)))\n",
    "[X[:,:,i]=f for (i,f) in adj_mats|>enumerate];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalize_factors(factors)\n",
    "    [factor ./ (map(norm, eachcol(factors[1])) |> permutedims) for factor in factors]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a variety of Fits for X. Distribute it so that we can keep using the notebook while this is executing.\n",
    "using Distributed\n",
    "f = @spawn begin\n",
    "    Fs = [F=nncp(X, r, verbose=false) for r in 1:1];\n",
    "    Cs = [CORCONDIER(X,F.factors,r) for (r,F) in enumerate(Fs)];\n",
    "    NCs = [CORCONDIER(X,F.factors|>normalize_factors,r) for (r,F) in enumerate(Fs)];\n",
    "    Fs, Cs, NCs\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when it's done\n",
    "Fs,Cs,NCs = fetch(f) .|> x->x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a while (and makes the PC hang for a little bit)\n",
    "\n",
    "R = TIMESTEPS # corcondia scales badly with R, so think before you run this\n",
    "cc = []\n",
    "for r in 1:R\n",
    "    try\n",
    "    @time F = nncp(ADJ, r; # R is the number of components ≈ number of communities\n",
    "        # I think Ciro spoke about optimising this number somewhere\n",
    "        # papers refer to it - core consistency\n",
    "        compute_error=true,\n",
    "        maxiter=1000,\n",
    "    );\n",
    "    push!(cc,(r,CORCONDIA(ADJ,F.factors,r)))\n",
    "    catch\n",
    "        ; # Ignore convergence issues etc\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plot([c[1] for c in cc], [c[2] for c in cc]) # Hmm. Can't have more communities than timesteps.\n",
    "# That's a bit weird.\n",
    "\n",
    "# function is so bouncy that it looks pretty suspect \n",
    "# corcondia performs badly with large R too, it seems\n",
    "\n",
    "# I'm beginning to suspect that it's just BS.\n",
    "\n",
    "# Maybe we should look at silhouette instead?\n",
    "\n",
    "# Hmm, since it's so bouncy, should we be run it a few times till we get the best score?\n",
    "# Optimise our random seed as well as R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc # Shouldn't R=1 always be perfect? I think something is wrong with our CORCONDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 3\n",
    "@time F = nncp(ADJ, R; # R is the number of components ≈ number of communities\n",
    "    \n",
    "    # I think Ciro spoke about optimising this number somewhere\n",
    "    # papers refer to it - core consistency\n",
    "    compute_error=true,\n",
    "    maxiter=1000,\n",
    ");\n",
    "# (each u*v*w[:,r] is a component; each of u,v,w is a factor)\n",
    "# n = 1000 -> 2 seconds\n",
    "# n = 10,000 -> bloody ages. severely limited by i/o - using 25% CPU only. has been like, 5 minutes. Am bored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.factors[3] |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.factors[1] # The largest numbers in each nodes correspond to the nodes most active in that community\n",
    "(u,v,w) = [f for f in F.factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plot(F.factors[3]) # You can see that some communities are inactive for the first few timesteps, as we'd expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.heatmap(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GraphPlot\n",
    "import Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function show_communities(X, F, r)\n",
    "    N = size(X)[1]\n",
    "    u = F.factors[1]\n",
    "    adj = X[:,:,size(X)[3]] |> BitArray\n",
    "    \n",
    "    if ((adj .| adj') .== adj) |> all\n",
    "        g = LightGraphs.Graph(adj)\n",
    "    else\n",
    "        g = LightGraphs.DiGraph(adj)\n",
    "    end\n",
    "    \n",
    "    colours = Colors.distinguishable_colors(r,Colors.colorant\"blue\")\n",
    "    nodefillarr = []\n",
    "    for n in 1:N\n",
    "        ind = findmax(u[n,:])[2]\n",
    "        push!(nodefillarr,colours[ind])\n",
    "    end\n",
    "    \n",
    "    if N > 300\n",
    "        proc = GraphPlot.gplothtml(g;nodefillc=nodefillarr)\n",
    "        print(\"http://blanthornpc:2015/\", split(proc.cmd.exec[2], \"/\")[3])\n",
    "    else\n",
    "        GraphPlot.gplot(g;nodefillc=nodefillarr)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_communities(ADJ, F, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undir = X -> begin X = BitArray(X); (X .| X') |> Array{Float64} end\n",
    "Xundir = similar(X)\n",
    "[Xundir[:,:,i] = undir(X[:,:,i]) for i in 1:size(X)[3]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The undirected graph seems to have better community detection, looking purely at the graphplot of the final graph.\n",
    "# TODO:\n",
    "# Try plotting factors[2] instead?\n",
    "r = 5\n",
    "Fundir = nncp(Xundir, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = show_communities(Xundir, Fundir, 5)\n",
    "# Start caddy in /tmp first\n",
    "print(\"http://blanthornpc:2015/\", split(proc.cmd.exec[2], \"/\")[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_communities(X, Fs[5], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_size = N ÷ R\n",
    "colours = Colors.distinguishable_colors(R,Colors.colorant\"blue\")\n",
    "nodefillarr = []\n",
    "for n in 1:N\n",
    "    ind = findmax(u[n,:])[2]\n",
    "    push!(nodefillarr,colours[ind])\n",
    "end\n",
    "GraphPlot.gplot(g;nodefillc=nodefillarr) # Sure, these kind of look like communities.\n",
    "# Eugh, if you put too many nodes in, GraphPlot gives up trying to colour them\n",
    "# Would be nice to colour nodes by strength of their association too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network interpretation\n",
    "\n",
    "Say you have a temporal adjacency matrix which you wish to decompose into $R$ communities:\n",
    "\n",
    "$$ a_{ijt} = \\Sigma_{r=1}^R u_{ir} v_{jr} w_{tr} $$\n",
    "\n",
    "NB: Each of $u,v,w$ is roughly (self) column-wise orthogonal - i.e, if an entry is large in one column of, e.g, u, then it will be smaller in another column of u.\n",
    "\n",
    "Therefore, for any edge $a_{ijt}$ to appear from node $j$ to $i$ at time $t$, it must be true that, for some $r$, $u_{ir} v_{jr} w_{tr} \\approx 1$.\n",
    "\n",
    "Ignoring the temporal factor for a moment, we can say that high-scoring nodes in $\\boldsymbol{u}_{r}$ are probably linked to by nodes in $\\boldsymbol{v}_{r}$ - all at once, i.e, each $r$ fuzzily denotes a community of nodes who link to each other: so, $u_{ir}$ denotes the strength\n",
    "\n",
    "$w_{tr}$ has a similar meaning: the highest scoring times for each $r$ means that the high-scoring nodes in $u$ and $v$ are \"turned on\" and have more outgoing or incoming links at those times.\n",
    "\n",
    "In an undirected graph, we expect $u$ and $v$ to look similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fiddling\n",
    "\n",
    "You don't need to pay attention to stuff below this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = u * v'\n",
    "@tensor a[i,j,k] = b[i,j] * w[k] + 0.001*z[i,j,k]\n",
    "\n",
    "fixme = ones(1:TIMESTEPS)\n",
    "@tensor testADJ[i,j,t] := base_adj[i,j] * fixme[t] + perms[i,j,t]\n",
    "\n",
    "# I really don't understand the difference between these expressions\n",
    "# Or why fixme is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show findfirst(x->false,[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays\n",
    "\n",
    "DIMS = [10,20,30]\n",
    "u,v,w = [round.(rand(dim)) for dim in DIMS] .|> hcat .|> SparseMatrixCSC # rounding should actually be done later\n",
    "z = rand(DIMS...)\n",
    "a = zeroes(DIMS...) # |> sparse # Error - sparse tensors aren't in Julia. Consider adopting https://github.com/JuliaTensors/SparseTensors.jl ?\n",
    "# Or we could write a wrapper for taco - https://github.com/tensor-compiler/taco\n",
    "@show a |> size\n",
    "\n",
    "# This is using Einstein's summation convention - the whole tensor a is generated by iterating through all possible pairs \n",
    "#@tensor a[i,j,k] = u[i] * v[j] * w[k]# + 0.001*z[i,j,k]\n",
    "for i in 1:DIMS[1], j in 1:DIMS[2], k in 1:DIMS[3]\n",
    "    a[i,j,k] = u[i] * v[j] * w[k]\n",
    "end\n",
    "@show mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_adj |> Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "Ns = 10 .^(1:0.1:4) .|> round .|> Int\n",
    "R = 10\n",
    "for n in Ns\n",
    "    ADJ = zeroes(n,n,1) # making this takes _AGES_\n",
    "    g = LightGraphs.watts_strogatz(n,4,0.01)\n",
    "    base_adj = g |> LightGraphs.adjacency_matrix;\n",
    "    ADJ[:,:,1] = base_adj\n",
    "\n",
    "    t = @elapsed F = nncp(ADJ, 1; # R is the number of components ≈ number of communities\n",
    "        # I think Ciro spoke about optimising this number somewhere\n",
    "        # papers refer to it - core consistency\n",
    "        compute_error=true,\n",
    "        maxiter=100,\n",
    "    );\n",
    "    push!(times,t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plotly()\n",
    "Plots.scatter(Ns,times;\n",
    "    xlabel = \"N\",\n",
    "    ylabel = \"Time taken / seconds\",\n",
    "    legend = :none,\n",
    "    yscale = :log10,\n",
    "    xscale = :log10\n",
    ")\n",
    "\n",
    "# tl;dr: power-law, N^k, where k is quite big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing R\n",
    "\n",
    "https://github.com/alessandrobessi/corcondia/blob/master/coreconsistency.py <- CORCONDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORCONDIA(ADJ,F.factors,10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "julia"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.x",
   "language": "julia",
   "name": "julia-1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
